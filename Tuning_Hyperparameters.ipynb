{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tuning Hyperparameters.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOk8cU/CUVsAyFFi/9gWxAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stemgene/Python-Diary/blob/master/Tuning_Hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIxBeBRbTfYM",
        "colab_type": "text"
      },
      "source": [
        "## SVM\n",
        "\n",
        "### Gaussian Kernel\n",
        "\n",
        "The parameters of Gaussian (rbf) kernel are `Gamma`(default gamma='scale'. 1 / (n_features * X.var())) and `C`(default C=1.0).\n",
        "\n",
        "Intuitively, the `gamma` parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
        "\n",
        "The C parameter trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM.\n",
        "\n",
        "The behavior of the model is very sensitive to the `gamma` parameter. If `gamma` is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with `C` will be able to prevent overfitting.\n",
        "\n",
        "When `gamma` is very small, the model is too constrained and cannot capture the complexity or “shape” of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.\n",
        "\n",
        "The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.\n",
        "![The first plot](https://scikit-learn.org/stable/_images/sphx_glr_plot_rbf_parameters_001.png)  \n",
        "\n",
        "The second plot is a heatmap of the classifier’s cross-validation accuracy as a function of C and gamma. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from 10^-3 to 10^3 is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.\n",
        "\n",
        "![2](https://scikit-learn.org/stable/_images/sphx_glr_plot_rbf_parameters_002.png)\n",
        "\n",
        "The best parameters are {'C': 1.0, 'gamma': 0.1} with a score of 0.97\n",
        "\n",
        "Note that the heat map plot has a special colorbar with a midpoint value close to the score values of the best performing models so as to make it easy to tell them apart in the blink of an eye.\n",
        "\n",
        "For intermediate values, we can see on the second plot that good models can be found on a diagonal of `C` and `gamma`. Smooth models (lower `gamma` values) can be made more complex by increasing the importance of classifying each point correctly (larger `C` values) hence the diagonal of good performing models.\n",
        "\n",
        "Finally one can also observe that for some intermediate values of gamma we get equally performing models when `C` becomes very large: it is not necessary to regularize by enforcing a larger margin. The radius of the RBF kernel alone acts as a good structural regularizer. In practice though it might still be interesting to simplify the decision function with a lower value of `C` so as to favor models that use less memory and that are faster to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSbmMaX5TaNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GridSearchCV\n",
        "# Kernels of Gaussian\n",
        "import time\n",
        "\n",
        "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
        "gammas = [0.001, 0.01, 0.1, 1, 10]\n",
        "param_grid = {'kernel':['rbf'], 'C': Cs, 'gamma':gammas}\n",
        "\n",
        "# for kernels of linear\n",
        "svc_rbf_lin = SVC()\n",
        "start = time.time()\n",
        "gridsearch_rbf_lin = GridSearchCV(svc_rbf_lin, param_grid, cv=10)\n",
        "gridsearch_rbf_lin.fit(X_train, y_train)\n",
        "print(gridsearch_rbf_lin.best_estimator_, '\\n',gridsearch_rbf_lin.best_score_)\n",
        "print(\"It cost\", round((time.time()-start),2)) \n",
        "\n",
        "\"\"\"\n",
        "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
        "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
        "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "  tol=0.001, verbose=False) \n",
        " 0.8692493946731235\n",
        "It cost 306.13"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09zv43L0ejhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RandomSearchCV\n",
        "# Kernels of linear and Gaussian\n",
        "import time\n",
        "\n",
        "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
        "gammas = [0.001, 0.01, 0.1, 1, 10]\n",
        "param_grid = {'kernel':['rbf'], 'C': Cs, 'gamma':gammas}\n",
        "\n",
        "# for kernels of linear\n",
        "svc_rbf = SVC()\n",
        "start = time.time()\n",
        "randomsearch_rbf = RandomizedSearchCV(svc_rbf, param_grid)\n",
        "randomsearch_rbf.fit(X_train, y_train)\n",
        "print(randsearch_rbf.best_estimator_, '\\n',randomsearch_rbf_lin.best_score_)\n",
        "print(\"It cost\", round((time.time()-start),2)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmqNSS43l3Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "iris = load_iris()\n",
        "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200, random_state=0)\n",
        "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
        "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
        "search = clf.fit(iris.data, iris.target)\n",
        "search.best_params_\n",
        "\"\"\"\n",
        "{'C': 2..., 'penalty': 'l1'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvy1svaUOpiv",
        "colab_type": "text"
      },
      "source": [
        "### Polynomial kernal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwgFwkj6LbK2",
        "colab_type": "text"
      },
      "source": [
        "Tuning polynomial kernel with grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcqspUsoLh7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = {'degree': [int(x) for x in np.linspace(start = 2, stop = 10, num = 9)]} # degree=1 is linear kernel\n",
        "svc_poly = SVC(kernel='poly')\n",
        "start = time.time()\n",
        "gridsearch_poly = GridSearchCV(svc_poly, param_grid)\n",
        "gridsearch_poly.fit(X_train, y_train)\n",
        "print(gridsearch_poly.best_estimator_, '\\n',gridsearch_poly.best_score_)\n",
        "print(\"It cost\", round((time.time()-start),2)) #485s in windows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2UZpP2AOte9",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVKTwu93OwSn",
        "colab_type": "text"
      },
      "source": [
        "A good place is the documentation [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) on the random forest in Scikit-Learn. This tells us the most important settings are the number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features). We could go read the research papers [paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) on the random forest and try to theorize the best hyperparameters, but a more efficient use of our time is just to try out a wide range of values and see what works! We will try adjusting the following set of hyperparameters:\n",
        "* n_estimators = number of trees in the foreset\n",
        "* max_features = max number of features considered for splitting a node\n",
        "* max_depth = max number of levels in each decision tree\n",
        "* min_samples_split = min number of data points placed in a node before the node is split\n",
        "* min_samples_leaf = min number of data points allowed in a leaf node\n",
        "* bootstrap = method for sampling data points (with or without replacement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbiKB4FrPN58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "\"\"\"\n",
        "{'bootstrap': [True, False],\n",
        " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        " 'max_features': ['auto', 'sqrt'],\n",
        " 'min_samples_leaf': [1, 2, 4],\n",
        " 'min_samples_split': [2, 5, 10],\n",
        " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
        "\"\"\"\n",
        "\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(train_features, train_labels)\n",
        "\n",
        "rf_random.best_params_\n",
        "\"\"\"\n",
        "{'bootstrap': True,\n",
        " 'max_depth': 70,\n",
        " 'max_features': 'auto',\n",
        " 'min_samples_leaf': 4,\n",
        " 'min_samples_split': 10,\n",
        " 'n_estimators': 400}\n",
        " \"\"\"\n",
        "\n",
        "# Evaluate Random Search\n",
        " def evaluate(model, test_features, test_labels):\n",
        "    predictions = model.predict(test_features)\n",
        "    errors = abs(predictions - test_labels)\n",
        "    mape = 100 * np.mean(errors / test_labels)\n",
        "    accuracy = 100 - mape\n",
        "    print('Model Performance')\n",
        "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
        "base_model.fit(train_features, train_labels)\n",
        "base_accuracy = evaluate(base_model, test_features, test_labels)\n",
        "\"\"\"\n",
        "Model Performance\n",
        "Average Error: 3.9199 degrees.\n",
        "Accuracy = 93.36%.\n",
        "\"\"\"\n",
        "\n",
        "best_random = rf_random.best_estimator_\n",
        "random_accuracy = evaluate(best_random, test_features, test_labels)\n",
        "\"\"\"\n",
        "Model Performance\n",
        "Average Error: 3.7152 degrees.\n",
        "Accuracy = 93.73%.\n",
        "\"\"\"\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
        "\"\"\"\n",
        "Improvement of 0.40%."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}